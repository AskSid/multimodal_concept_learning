{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Dataset & Collator Checks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook exercises the ImageNet-based multimodal dataset and collator setup used in training.\n",
        "Each cell mirrors the same pathways the training script relies on (paths from the default config).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "\n",
        "from src.multimodal.multimodal_training_config import MultimodalTrainingConfig\n",
        "from src.multimodal.multimodal_training import load_multimodal_dataset\n",
        "from src.datasets.imagenet.imagenet_dataset import ImageNetDataset, MultimodalCollator\n",
        "from src.utils import create_transforms\n",
        "from transformers import AutoTokenizer, AutoImageProcessor\n",
        "from torch.utils.data import DataLoader, Subset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load default multimodal config (assumes repository paths are valid on this machine)\n",
        "config = MultimodalTrainingConfig.from_params({})\n",
        "pprint({\n",
        "    'mapping_path': config.mapping_path,\n",
        "    'extra_mapping_path': config.extra_mapping_path,\n",
        "    'image_root': config.image_root,\n",
        "    'vision_model_name': config.vision_model_name,\n",
        "    'language_model_name': config.language_model_name,\n",
        "    'num_vision_tokens': config.num_vision_tokens,\n",
        "    'train_transforms': config.train_transforms,\n",
        "    'val_transforms': config.val_transforms,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity-check expected filesystem inputs\n",
        "assert os.path.exists(config.mapping_path), f'Missing mapping CSV at {config.mapping_path}'\n",
        "if config.extra_mapping_path:\n",
        "    assert os.path.exists(config.extra_mapping_path), f'Missing extra mapping CSV at {config.extra_mapping_path}'\n",
        "assert os.path.exists(config.image_root), f'Missing image root at {config.image_root}'\n",
        "print('All required paths located.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the raw ImageNet mapping as a dataset (no train/val split)\n",
        "full_dataset = ImageNetDataset(\n",
        "    config.mapping_path,\n",
        "    config.image_root,\n",
        "    transform=None,\n",
        "    return_synset=True,\n",
        ")\n",
        "print(f'Total rows: {len(full_dataset)} | Unique classes: {full_dataset.num_classes}')\n",
        "print('First three entries:', full_dataset.dataset[:3])\n",
        "print('First five class labels:', full_dataset.unique_labels[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect an arbitrary sample\n",
        "sample_idx = random.randint(0, len(full_dataset) - 1)\n",
        "sample_image, sample_label = full_dataset[sample_idx]\n",
        "print(f'Random sample index: {sample_idx} | Class name: {sample_label}')\n",
        "print('Image type:', type(sample_image))\n",
        "if hasattr(sample_image, 'size'):\n",
        "    print('Image size:', sample_image.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reuse the training helper to produce the stratified train/val subsets\n",
        "train_dataset, val_dataset = load_multimodal_dataset(config)\n",
        "print(f'Train subset: {len(train_dataset)} rows | Val subset: {len(val_dataset)} rows')\n",
        "print('Train subset retains class metadata?', hasattr(train_dataset, 'unique_labels'))\n",
        "print('Example labels:', train_dataset.unique_labels[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assemble tokenizer & image processor for the collator\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.language_model_name,\n",
        "    use_fast=config.use_fast_tokenizer,\n",
        ")\n",
        "image_processor = AutoImageProcessor.from_pretrained(config.vision_model_name)\n",
        "collator = MultimodalCollator(\n",
        "    image_processor=image_processor,\n",
        "    tokenizer=tokenizer,\n",
        "    num_vision_tokens=config.num_vision_tokens,\n",
        "    prompt_template=config.prompt_template,\n",
        "    all_class_names=train_dataset.unique_labels,\n",
        ")\n",
        "print('Tokenizer vocab size:', tokenizer.vocab_size)\n",
        "print('Vision processor size:', image_processor.size if hasattr(image_processor, 'size') else 'n/a')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a small inspection DataLoader\n",
        "inspection_indices = list(range(min(4, len(train_dataset))))\n",
        "inspection_subset = Subset(train_dataset, inspection_indices)\n",
        "inspection_loader = DataLoader(inspection_subset, batch_size=2, shuffle=False, collate_fn=collator)\n",
        "batch = next(iter(inspection_loader))\n",
        "print('Batch keys:', batch.keys())\n",
        "for key, value in batch.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"{key}: shape={tuple(value.shape)} dtype={value.dtype}\")\n",
        "    else:\n",
        "        print(f\"{key}: type={type(value)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate that masking matches expectations\n",
        "labels = batch['labels']\n",
        "vision_token_block = labels[:, :config.num_vision_tokens]\n",
        "assert torch.all(vision_token_block == -100), 'Vision positions should be masked with -100'\n",
        "answer_mask = labels != -100\n",
        "answer_counts = answer_mask.sum(dim=1)\n",
        "print('Answer tokens per example:', answer_counts.tolist())\n",
        "assert torch.all(answer_counts > 0), 'Each prompt should contain supervised answer tokens'\n",
        "print('Label masking checks passed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode the textual portion (excluding vision token padding) for a qualitative check\n",
        "text_only_ids = batch['input_ids'][:, config.num_vision_tokens:]\n",
        "decoded_texts = tokenizer.batch_decode(text_only_ids, skip_special_tokens=False)\n",
        "for idx, text in enumerate(decoded_texts):\n",
        "    print(f'Example {idx}: {text}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}