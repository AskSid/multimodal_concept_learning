{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b7c29f0",
      "metadata": {},
      "source": [
        "# Multimodal Dataset & Collator Checks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdda60d3",
      "metadata": {},
      "source": [
        "This notebook exercises the ImageNet-based multimodal dataset and collator setup used in training.\n",
        "Each cell mirrors the same pathways the training script relies on (paths from the default config).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "52ef5847",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "from pprint import pprint\n",
        "\n",
        "# Add the project root to Python path\n",
        "project_root = '/users/sboppana/data/sboppana/multimodal_concept_learning'\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "import torch\n",
        "\n",
        "from src.multimodal.multimodal_training_config import MultimodalTrainingConfig\n",
        "from src.multimodal.multimodal_training import load_multimodal_dataset\n",
        "from src.datasets.imagenet.imagenet_dataset import ImageNetDataset, MultimodalCollator\n",
        "\n",
        "# Try to import create_multimodal_transforms, with fallback if it fails\n",
        "try:\n",
        "    from src.utils import create_multimodal_transforms\n",
        "except ImportError:\n",
        "    # Fallback: define the function locally if import fails\n",
        "    from torchvision import transforms\n",
        "    def create_multimodal_transforms(config, is_train=True):\n",
        "        \"\"\"Image transforms for multimodal training.\"\"\"\n",
        "        if is_train:\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "from transformers import AutoTokenizer, AutoImageProcessor\n",
        "from torch.utils.data import DataLoader, Subset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4faaf17",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "42b5b7fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'extra_mapping_path': '/users/sboppana/data/sboppana/multimodal_concept_mapping/data/imagenet100_v2/train_mapping_separate.csv',\n",
            " 'image_root': '/users/sboppana/data/sboppana/multimodal_concept_mapping/data/imagenet/train',\n",
            " 'language_model_name': 'google/gemma-3-1b-it',\n",
            " 'mapping_path': '/users/sboppana/data/sboppana/multimodal_concept_mapping/data/imagenet100_v2/train_mapping.csv',\n",
            " 'num_vision_tokens': 197,\n",
            " 'vision_model_name': 'google/vit-base-patch16-224-in21k'}\n"
          ]
        }
      ],
      "source": [
        "# Load default multimodal config (assumes repository paths are valid on this machine)\n",
        "config = MultimodalTrainingConfig.from_params({})\n",
        "pprint({\n",
        "    'mapping_path': config.mapping_path,\n",
        "    'extra_mapping_path': config.extra_mapping_path,\n",
        "    'image_root': config.image_root,\n",
        "    'vision_model_name': config.vision_model_name,\n",
        "    'language_model_name': config.language_model_name,\n",
        "    'num_vision_tokens': config.num_vision_tokens,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All required paths located.\n"
          ]
        }
      ],
      "source": [
        "# Sanity-check expected filesystem inputs\n",
        "assert os.path.exists(config.mapping_path), f'Missing mapping CSV at {config.mapping_path}'\n",
        "if config.extra_mapping_path:\n",
        "    assert os.path.exists(config.extra_mapping_path), f'Missing extra mapping CSV at {config.extra_mapping_path}'\n",
        "assert os.path.exists(config.image_root), f'Missing image root at {config.image_root}'\n",
        "print('All required paths located.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'class_name'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/data/sboppana/multimodal_concept_learning/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'class_name'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the raw ImageNet mapping as a dataset (no train/val split)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m full_dataset = \u001b[43mImageNetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapping_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_synset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTotal rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(full_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Unique classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_dataset.num_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFirst three entries:\u001b[39m\u001b[33m'\u001b[39m, full_dataset.dataset[:\u001b[32m3\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/data/sboppana/multimodal_concept_learning/src/datasets/imagenet/imagenet_dataset.py:28\u001b[39m, in \u001b[36mImageNetDataset.__init__\u001b[39m\u001b[34m(self, mapping_csv_path, data_dir, transform, return_synset)\u001b[39m\n\u001b[32m     24\u001b[39m image_path = os.path.join(\u001b[38;5;28mself\u001b[39m.data_dir, row[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_synset:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# For multimodal, return class_name\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     class_name = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclass_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m.dataset.append((image_path, class_name))\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# For vision, return target_synset\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/data/sboppana/multimodal_concept_learning/.venv/lib/python3.13/site-packages/pandas/core/series.py:1133\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/data/sboppana/multimodal_concept_learning/.venv/lib/python3.13/site-packages/pandas/core/series.py:1249\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/data/sboppana/multimodal_concept_learning/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'class_name'"
          ]
        }
      ],
      "source": [
        "# Load the raw ImageNet mapping as a dataset (no train/val split)\n",
        "full_dataset = ImageNetDataset(\n",
        "    config.mapping_path,\n",
        "    config.image_root,\n",
        "    transform=None,\n",
        "    return_synset=True,\n",
        ")\n",
        "print(f'Total rows: {len(full_dataset)} | Unique classes: {full_dataset.num_classes}')\n",
        "print('First three entries:', full_dataset.dataset[:3])\n",
        "print('First five class labels:', full_dataset.unique_labels[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect an arbitrary sample\n",
        "sample_idx = random.randint(0, len(full_dataset) - 1)\n",
        "sample_image, sample_label = full_dataset[sample_idx]\n",
        "print(f'Random sample index: {sample_idx} | Class name: {sample_label}')\n",
        "print('Image type:', type(sample_image))\n",
        "if hasattr(sample_image, 'size'):\n",
        "    print('Image size:', sample_image.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reuse the training helper to produce the stratified train/val subsets\n",
        "train_dataset, val_dataset = load_multimodal_dataset(config)\n",
        "print(f'Train subset: {len(train_dataset)} rows | Val subset: {len(val_dataset)} rows')\n",
        "print('Train subset retains class metadata?', hasattr(train_dataset, 'unique_labels'))\n",
        "print('Example labels:', train_dataset.unique_labels[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assemble tokenizer & image processor for the collator\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.language_model_name,\n",
        "    use_fast=config.use_fast_tokenizer,\n",
        ")\n",
        "image_processor = AutoImageProcessor.from_pretrained(config.vision_model_name)\n",
        "collator = MultimodalCollator(\n",
        "    image_processor=image_processor,\n",
        "    tokenizer=tokenizer,\n",
        "    num_vision_tokens=config.num_vision_tokens,\n",
        "    prompt_template=config.prompt_template,\n",
        "    all_class_names=train_dataset.unique_labels,\n",
        ")\n",
        "print('Tokenizer vocab size:', tokenizer.vocab_size)\n",
        "print('Vision processor size:', image_processor.size if hasattr(image_processor, 'size') else 'n/a')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a small inspection DataLoader\n",
        "inspection_indices = list(range(min(4, len(train_dataset))))\n",
        "inspection_subset = Subset(train_dataset, inspection_indices)\n",
        "inspection_loader = DataLoader(inspection_subset, batch_size=2, shuffle=False, collate_fn=collator)\n",
        "batch = next(iter(inspection_loader))\n",
        "print('Batch keys:', batch.keys())\n",
        "for key, value in batch.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"{key}: shape={tuple(value.shape)} dtype={value.dtype}\")\n",
        "    else:\n",
        "        print(f\"{key}: type={type(value)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate that masking matches expectations\n",
        "labels = batch['labels']\n",
        "vision_token_block = labels[:, :config.num_vision_tokens]\n",
        "assert torch.all(vision_token_block == -100), 'Vision positions should be masked with -100'\n",
        "answer_mask = labels != -100\n",
        "answer_counts = answer_mask.sum(dim=1)\n",
        "print('Answer tokens per example:', answer_counts.tolist())\n",
        "assert torch.all(answer_counts > 0), 'Each prompt should contain supervised answer tokens'\n",
        "print('Label masking checks passed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode the textual portion (excluding vision token padding) for a qualitative check\n",
        "text_only_ids = batch['input_ids'][:, config.num_vision_tokens:]\n",
        "decoded_texts = tokenizer.batch_decode(text_only_ids, skip_special_tokens=False)\n",
        "for idx, text in enumerate(decoded_texts):\n",
        "    print(f'Example {idx}: {text}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
